{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Recognition Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "import seaborn; seaborn.set()\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 238, 319, 8)       152       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 236, 318, 16)      784       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 118, 159, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 116, 158, 16)      1552      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 114, 157, 8)       776       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 57, 78, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 57, 78, 8)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 35568)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               4552832   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 1161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,557,257\n",
      "Trainable params: 4,557,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 2), activation='relu', input_shape=(240, 320, 3)))\n",
    "model.add(Conv2D(16, (3, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16, (3, 2), activation='relu'))\n",
    "model.add(Conv2D(8, (3, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(9, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image and Config Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads an image and its cfg file\n",
    "def importImage(path, X, Y):\n",
    "    #Load the image\n",
    "    img = load_img(path)\n",
    "    arr = img_to_array(img)\n",
    "    \n",
    "    #Load the cfg\n",
    "    configFile = open(path[:-4] + '.cfg')\n",
    "    configRaw = configFile.read()\n",
    "    config = json.loads(configRaw, object_hook=lambda d: SimpleNamespace(**d)) \n",
    "    \n",
    "    cfgArr = [config.valid, config.x1, config.y1, config.x2, config.y2, config.x3, config.y3, config.x4, config.y4]\n",
    "    X.append(arr)\n",
    "    Y.append(cfgArr)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAllTrainingData(data_path):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for root, dirs, files in os.walk(data_path, topdown=False):\n",
    "        for f in files:\n",
    "            if (f.endswith(\".jpg\")):\n",
    "                X, Y = importImage(os.path.join(root, f), X, Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainingValidation(X, Y, split_size=0.2):\n",
    "    if len(X) == len(Y):\n",
    "        splitCount = int(len(X) * split_size)\n",
    "        \n",
    "        used_indices = []\n",
    "        \n",
    "        X_set = []\n",
    "        X_val = []\n",
    "        Y_set = []\n",
    "        Y_val = []\n",
    "        \n",
    "        while len(X_val) < splitCount:\n",
    "            index = random.randint(0, len(X)-1)\n",
    "            if index not in used_indices:\n",
    "                X_val.append(X[index])\n",
    "                Y_val.append(Y[index])\n",
    "                used_indices.append(index)\n",
    "        \n",
    "        for i in range(0, len(X)-1):\n",
    "            if i not in used_indices:\n",
    "                X_set.append(X[i])\n",
    "                Y_set.append(Y[i])\n",
    "                \n",
    "        return np.array(X_set), np.array(Y_set), np.array(X_val), np.array(Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training Data and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   ...\n",
      "   [ 3.  3.  3.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  [[ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   ...\n",
      "   [ 3.  3.  3.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  [[ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   ...\n",
      "   [ 3.  3.  3.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[22. 22. 22.]\n",
      "   [23. 23. 23.]\n",
      "   [24. 24. 24.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[20. 20. 20.]\n",
      "   [21. 21. 21.]\n",
      "   [22. 22. 22.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[18. 18. 18.]\n",
      "   [19. 19. 19.]\n",
      "   [20. 20. 20.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]]\n",
      "\n",
      "\n",
      " [[[ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[ 7.  7.  7.]\n",
      "   [ 7.  7.  7.]\n",
      "   [ 7.  7.  7.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[ 5.  5.  5.]\n",
      "   [ 5.  5.  5.]\n",
      "   [ 5.  5.  5.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[19. 19. 19.]\n",
      "   [22. 22. 22.]\n",
      "   [24. 24. 24.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[21. 21. 21.]\n",
      "   [23. 23. 23.]\n",
      "   [26. 26. 26.]\n",
      "   ...\n",
      "   [ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]]\n",
      "\n",
      "  [[22. 22. 22.]\n",
      "   [25. 25. 25.]\n",
      "   [27. 27. 27.]\n",
      "   ...\n",
      "   [12. 12. 12.]\n",
      "   [12. 12. 12.]\n",
      "   [12. 12. 12.]]]\n",
      "\n",
      "\n",
      " [[[ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[38. 38. 38.]\n",
      "   [39. 39. 39.]\n",
      "   [39. 39. 39.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[38. 38. 38.]\n",
      "   [38. 38. 38.]\n",
      "   [39. 39. 39.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[37. 37. 37.]\n",
      "   [38. 38. 38.]\n",
      "   [38. 38. 38.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   ...\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]]\n",
      "\n",
      "  [[ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   ...\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]]\n",
      "\n",
      "  [[ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   [ 8.  8.  8.]\n",
      "   ...\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]\n",
      "   [ 6.  6.  6.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[40. 40. 40.]\n",
      "   [42. 42. 42.]\n",
      "   [44. 44. 44.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[40. 40. 40.]\n",
      "   [42. 42. 42.]\n",
      "   [44. 44. 44.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]\n",
      "\n",
      "  [[40. 40. 40.]\n",
      "   [42. 42. 42.]\n",
      "   [44. 44. 44.]\n",
      "   ...\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]\n",
      "   [ 4.  4.  4.]]]\n",
      "\n",
      "\n",
      " [[[22. 22. 22.]\n",
      "   [23. 23. 23.]\n",
      "   [25. 25. 25.]\n",
      "   ...\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  [[23. 23. 23.]\n",
      "   [24. 24. 24.]\n",
      "   [25. 25. 25.]\n",
      "   ...\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  [[23. 23. 23.]\n",
      "   [24. 24. 24.]\n",
      "   [25. 25. 25.]\n",
      "   ...\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[50. 50. 50.]\n",
      "   [49. 49. 49.]\n",
      "   [48. 48. 48.]\n",
      "   ...\n",
      "   [21. 21. 21.]\n",
      "   [22. 22. 22.]\n",
      "   [23. 23. 23.]]\n",
      "\n",
      "  [[52. 52. 52.]\n",
      "   [51. 51. 51.]\n",
      "   [50. 50. 50.]\n",
      "   ...\n",
      "   [20. 20. 20.]\n",
      "   [20. 20. 20.]\n",
      "   [21. 21. 21.]]\n",
      "\n",
      "  [[55. 55. 55.]\n",
      "   [54. 54. 54.]\n",
      "   [53. 53. 53.]\n",
      "   ...\n",
      "   [19. 19. 19.]\n",
      "   [19. 19. 19.]\n",
      "   [19. 19. 19.]]]\n",
      "\n",
      "\n",
      " [[[18. 18. 18.]\n",
      "   [13. 13. 13.]\n",
      "   [ 9.  9.  9.]\n",
      "   ...\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  [[18. 18. 18.]\n",
      "   [13. 13. 13.]\n",
      "   [ 9.  9.  9.]\n",
      "   ...\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  [[18. 18. 18.]\n",
      "   [13. 13. 13.]\n",
      "   [ 9.  9.  9.]\n",
      "   ...\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]\n",
      "   [ 2.  2.  2.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[46. 46. 46.]\n",
      "   [48. 48. 48.]\n",
      "   [50. 50. 50.]\n",
      "   ...\n",
      "   [12. 12. 12.]\n",
      "   [12. 12. 12.]\n",
      "   [12. 12. 12.]]\n",
      "\n",
      "  [[46. 46. 46.]\n",
      "   [48. 48. 48.]\n",
      "   [50. 50. 50.]\n",
      "   ...\n",
      "   [16. 16. 16.]\n",
      "   [16. 16. 16.]\n",
      "   [16. 16. 16.]]\n",
      "\n",
      "  [[46. 46. 46.]\n",
      "   [48. 48. 48.]\n",
      "   [50. 50. 50.]\n",
      "   ...\n",
      "   [19. 19. 19.]\n",
      "   [19. 19. 19.]\n",
      "   [19. 19. 19.]]]]\n",
      "[[  1  39  51 279  56  29 206 288 208]\n",
      " [  1  34  45 279  48  25 200 287 202]\n",
      " [  1  28  44 284  23  40 197 293 186]\n",
      " [  1  19  26 299  28  25 201 300 196]\n",
      " [  1  40  35 285  35  43 189 286 184]\n",
      " [  1  41  88 243  29  72 213 282 156]\n",
      " [  1  40  69 255  62   3 209 309 186]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  20  38 284  26   5 201 302 203]\n",
      " [  1  35  63 266  14  60 202 295 161]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  39  84 227  69  37 204 247 186]\n",
      " [  1  28  59 277  42  24 215 299 204]\n",
      " [  1  22  90 221  24  53 225 285 137]\n",
      " [  1  89  40 278  67  54 148 259 195]\n",
      " [  1  27  47 272  29  22 202 298 185]\n",
      " [  1  22  37 287  36  20 205 295 202]\n",
      " [  1  39  56 268  39  47 193 273 186]\n",
      " [  1  21  61 249  19  47 199 271 163]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  24  42 280  25  34 201 293 184]\n",
      " [  1  28  92 231  22  72 220 281 143]\n",
      " [  1  48  50 261  42  51 184 273 172]\n",
      " [  1  30  40 282  19  41 196 297 176]\n",
      " [  1  44  40 280  41  43 187 285 187]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  33  78 255  48  42 218 283 189]\n",
      " [  1  52  45 263  39  49 174 268 173]\n",
      " [  1  66  66 267  86  41 190 266 216]\n",
      " [  1  30  34 279  15  32 188 301 172]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  49  83 261  65  45 217 288 200]\n",
      " [  1  52  46 272  48  50 183 275 184]\n",
      " [  1 118  13 277 123  29 100 229 230]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  37  37 277  38  31 187 284 187]\n",
      " [  1  32  66 247  34  48 199 272 168]\n",
      " [  1  47  50 251  47  46 178 259 172]\n",
      " [  1  71  22 278  69  33 149 257 199]\n",
      " [  1  38  93 242  10  82 223 305 136]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  39  44 291  23  36 197 311 188]\n",
      " [  1  29  54 288  51  28 212 290 216]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  45  42 274  38  40 182 270 185]\n",
      " [  1 106  45 283 107  48 144 242 226]\n",
      " [  1  37  75 245  21  72 203 279 151]\n",
      " [  1  44  37 248  32  39 161 248 161]\n",
      " [  1  31  35 290  35  30 202 292 190]\n",
      " [  1  52  66 281  61  51 210 282 203]\n",
      " [  1  29  30 298  34  29 197 289 196]\n",
      " [  1  48  41 283  37  50 184 283 185]\n",
      " [  1  50  42 277  46  45 181 273 190]\n",
      " [  1  37  41 293  35  33 198 299 200]\n",
      " [  1  26  84 245  22  65 219 283 158]\n",
      " [  1  28  87 229  28  58 218 278 149]\n",
      " [  1  98  23 291  85  45 138 267 207]\n",
      " [  1  56  54 285  51  52 201 300 193]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  75  28 272  64  44 148 257 188]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  36  34 266  33  33 178 270 178]\n",
      " [  1  29  32 280  33  29 191 287 186]\n",
      " [  1  31  61 272  47  35 216 292 196]\n",
      " [  1  27  68 259  13  59 211 298 157]\n",
      " [  1  63  31 293  65  34 176 284 210]\n",
      " [  1  24  48 276  41  31 213 291 193]\n",
      " [  1  58  48 280  41  67 190 291 175]\n",
      " [  1  18  75 246   6  61 219 295 146]\n",
      " [  1  91  15 300  83  39 150 277 216]\n",
      " [  1  47  50 252  43  53 181 263 166]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  88  19 297  93  26 148 270 229]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  57  45 287  40  60 190 296 182]\n",
      " [  1  40  33 291  33  40 193 301 187]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1  45  22 290  16  46 170 294 171]\n",
      " [  1  33  39 296  38  36 201 298 202]\n",
      " [  1  48  38 277  41  46 183 283 182]\n",
      " [  1  41  43 284  37  45 193 291 189]]\n",
      "(21, 240, 320, 3)\n",
      "(21, 9)\n"
     ]
    }
   ],
   "source": [
    "X, Y = loadAllTrainingData(\"../Traing Data Preprocessing/Training Data/Grid\")\n",
    "X, Y, X_val, Y_val = splitTrainingValidation(X, Y, split_size=0.2)\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 2s 172ms/step - loss: 22536.6426 - accuracy: 0.2442 - val_loss: 7991.8477 - val_accuracy: 0.1905\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 4981.6812 - accuracy: 0.3721 - val_loss: 5133.9702 - val_accuracy: 0.3810\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 3424.8193 - accuracy: 0.5581 - val_loss: 3650.7739 - val_accuracy: 0.5238\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 2307.4138 - accuracy: 0.4767 - val_loss: 3286.5283 - val_accuracy: 0.5238\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 2048.8518 - accuracy: 0.6512 - val_loss: 3172.0420 - val_accuracy: 0.5714\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 1218.0107 - accuracy: 0.6163 - val_loss: 2847.6411 - val_accuracy: 0.6190\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 713.5649 - accuracy: 0.5000 - val_loss: 2488.7310 - val_accuracy: 0.5714\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 394.0206 - accuracy: 0.5814 - val_loss: 2318.3276 - val_accuracy: 0.5714\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 294.7603 - accuracy: 0.6395 - val_loss: 2325.2017 - val_accuracy: 0.5714\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 228.9947 - accuracy: 0.6047 - val_loss: 2331.6406 - val_accuracy: 0.6190\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 217.7856 - accuracy: 0.5465 - val_loss: 2318.1580 - val_accuracy: 0.6667\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 172.5332 - accuracy: 0.6163 - val_loss: 2382.3833 - val_accuracy: 0.5714\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 122.3729 - accuracy: 0.6744 - val_loss: 2373.2317 - val_accuracy: 0.5714\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 109.0611 - accuracy: 0.5581 - val_loss: 2366.1533 - val_accuracy: 0.5238\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 101.3580 - accuracy: 0.6047 - val_loss: 2366.8357 - val_accuracy: 0.5238\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 89.3507 - accuracy: 0.7442 - val_loss: 2364.4824 - val_accuracy: 0.6190\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 96.3978 - accuracy: 0.6628 - val_loss: 2364.7083 - val_accuracy: 0.6667\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 102.3876 - accuracy: 0.6860 - val_loss: 2385.8926 - val_accuracy: 0.5714\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 112.6058 - accuracy: 0.6744 - val_loss: 2370.0991 - val_accuracy: 0.5238\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 104.1028 - accuracy: 0.6047 - val_loss: 2449.4729 - val_accuracy: 0.5238\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 105.0421 - accuracy: 0.6047 - val_loss: 2415.4753 - val_accuracy: 0.5714\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 84.4970 - accuracy: 0.7326 - val_loss: 2460.7866 - val_accuracy: 0.5238\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 121.6579 - accuracy: 0.6163 - val_loss: 2477.5049 - val_accuracy: 0.5238\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 143.3193 - accuracy: 0.5581 - val_loss: 2545.2444 - val_accuracy: 0.5238\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 115.6388 - accuracy: 0.6163 - val_loss: 2552.5474 - val_accuracy: 0.5238\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 125.5773 - accuracy: 0.7442 - val_loss: 2477.0269 - val_accuracy: 0.5238\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 106.1472 - accuracy: 0.6512 - val_loss: 2563.8962 - val_accuracy: 0.5238\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 109.6722 - accuracy: 0.5581 - val_loss: 2493.4568 - val_accuracy: 0.5238\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 107.6469 - accuracy: 0.6047 - val_loss: 2419.9028 - val_accuracy: 0.5238\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 91.4841 - accuracy: 0.6628 - val_loss: 2476.8765 - val_accuracy: 0.5238\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 122.6432 - accuracy: 0.6163 - val_loss: 2533.8208 - val_accuracy: 0.5714\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 139.6125 - accuracy: 0.6512 - val_loss: 2414.7263 - val_accuracy: 0.5238\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 119.6205 - accuracy: 0.6628 - val_loss: 2419.3691 - val_accuracy: 0.5714\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 203.2771 - accuracy: 0.6395 - val_loss: 2423.4702 - val_accuracy: 0.5714\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 227.0060 - accuracy: 0.7326 - val_loss: 2779.5376 - val_accuracy: 0.4762\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 216.6240 - accuracy: 0.6628 - val_loss: 2348.4270 - val_accuracy: 0.5714\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 115.1667 - accuracy: 0.6512 - val_loss: 2379.7739 - val_accuracy: 0.5714\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 87.0235 - accuracy: 0.7093 - val_loss: 2449.9880 - val_accuracy: 0.6190\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 84.2923 - accuracy: 0.6860 - val_loss: 2457.8992 - val_accuracy: 0.5714\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 86.9854 - accuracy: 0.7093 - val_loss: 2509.4170 - val_accuracy: 0.6190\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 78.0943 - accuracy: 0.6512 - val_loss: 2490.1707 - val_accuracy: 0.5714\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 73.8931 - accuracy: 0.6395 - val_loss: 2484.8101 - val_accuracy: 0.5714\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 64.6748 - accuracy: 0.6860 - val_loss: 2430.1616 - val_accuracy: 0.5238\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 90.0704 - accuracy: 0.6977 - val_loss: 2506.4167 - val_accuracy: 0.5714\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 79.9327 - accuracy: 0.6977 - val_loss: 2532.3784 - val_accuracy: 0.5238\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 2s 158ms/step - loss: 98.1043 - accuracy: 0.6744 - val_loss: 2670.1711 - val_accuracy: 0.6190\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 109.8504 - accuracy: 0.6628 - val_loss: 2555.7363 - val_accuracy: 0.6667\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 83.6035 - accuracy: 0.6628 - val_loss: 2530.9792 - val_accuracy: 0.6190\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 74.7972 - accuracy: 0.6512 - val_loss: 2492.4014 - val_accuracy: 0.5714\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 92.1803 - accuracy: 0.6860 - val_loss: 2497.7891 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c1d3534c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#es = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.01)\n",
    "\n",
    "model.fit(X, Y, epochs=50, validation_data=(X_val, Y_val), batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Random Images\n",
    "\n",
    "Try changing the sample number below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 49, 43, 306, 45, 48, 207, 310, 207]]\n",
      "[[1, 41, 43, 284, 37, 45, 193, 291, 189]]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "X_test, Y_test = importImage(\"../Traing Data Preprocessing/Training Data/Grid/sample_98.jpg\", X_test, Y_test)\n",
    "X_test = np.array(X_test)\n",
    "Y_predict = model.predict(X_test)\n",
    "Y_predict = [[round(Y_predict[0][0]), round(Y_predict[0][1]), round(Y_predict[0][2]), round(Y_predict[0][3]), round(Y_predict[0][4]), round(Y_predict[0][5]), round(Y_predict[0][6]), round(Y_predict[0][7]), round(Y_predict[0][8])]]\n",
    "print(Y_predict)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
